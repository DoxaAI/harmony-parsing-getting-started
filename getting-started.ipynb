{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Harmony**: Questionnaire Parsing Algorithm Improvement Challenge\n",
    "\n",
    "**NLP challenge** | [Visit the challenge page](https://doxaai.com/competition/harmony-parsing)\n",
    "\n",
    "Your challenge is to develop an improved algorithm for identifying mental health survey questions and selectable answers in plain text that can be integrated into the [Harmony tool](https://harmonydata.ac.uk/developer-guide/).\n",
    "\n",
    "This Jupyter notebook will introduce you to the challenge and guide you through the process of making your first submission to the [DOXA AI platform](https://doxaai.com/competition/harmony-parsing).\n",
    "\n",
    "**Before you get started, make sure to [sign up for an account](https://doxaai.com/sign-up) if you do not already have one and [enrol to take part](https://doxaai.com/competition/harmony-parsing) in the challenge.**\n",
    "\n",
    "**If you have any questions, feel free to ask them in the [Harmony community Discord server](https://discord.com/invite/harmonydata).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and importing useful packages\n",
    "\n",
    "Before you get started, please make sure you have [PyTorch](https://pytorch.org/get-started/locally/) installed in your Python environment. If you do not have `pandas`, `transformers` or `intervaltree`, uncomment the code in the following cell to install them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"pandas>=2.2.2\" \"transformers>=4.43.1\" \"intervaltree>=3.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of the DOXA CLI\n",
    "%pip install -U doxa-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if we do not already have it\n",
    "if not os.path.exists(\"data\"):\n",
    "    !curl https://raw.githubusercontent.com/DoxaAI/harmony-parsing-getting-started/main/data/train_raw.txt --create-dirs --output data/train_raw.txt\n",
    "    !curl https://raw.githubusercontent.com/DoxaAI/harmony-parsing-getting-started/main/data/train_clean.txt --output data/train_clean.txt\n",
    "    !curl https://raw.githubusercontent.com/DoxaAI/harmony-parsing-getting-started/main/data/train_labels.json --output data/train_labels.json\n",
    "\n",
    "if not os.path.exists(\"submission\"):\n",
    "    !curl https://raw.githubusercontent.com/DoxaAI/harmony-parsing-getting-started/main/submission/competition.py --create-dirs --output submission/competition.py\n",
    "    !curl https://raw.githubusercontent.com/DoxaAI/harmony-parsing-getting-started/main/submission/doxa.yaml --output submission/doxa.yaml\n",
    "    !curl https://raw.githubusercontent.com/DoxaAI/harmony-parsing-getting-started/main/submission/run.py --output submission/run.py\n",
    "\n",
    "with open(\"data/train_raw.txt\") as f:\n",
    "    raw_train = f.read()\n",
    "\n",
    "with open(\"data/train_clean.txt\") as g:\n",
    "    clean_train = g.read()\n",
    "\n",
    "with open(\"data/train_labels.json\") as h:\n",
    "    labels_train = json.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "\n",
    "Let's get started by taking a look at what the data looks like. The data comes in two forms:\n",
    "- **The raw plain text** where questions and answers have been manually tagged with `<q>`/`</q>` and `<a>`/`</a>` by the Harmony team\n",
    "- **A clean version** where the tags have been removed (with the question and answer ranges provided separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_train[:515])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_train[:451])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `labels_train` dictionary has starting indexes (inclusive) and ending indexes (exclusive) for the clean text that correspond to the tagged questions and answers in the raw text. For example, to pick out the first question in the raw text `\"I'm afraid that I might injury myself if I exercise\"`, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = labels_train[\"q\"][0]\n",
    "\n",
    "clean_train[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it significantly faster to query whether a word in a certain range is a question or an answer, we will build up two interval trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "tree_q = IntervalTree(\n",
    "    Interval(start, end) for start, end in labels_train[\"q\"] if start != end\n",
    ")\n",
    "\n",
    "tree_a = IntervalTree(\n",
    "    Interval(start, end) for start, end in labels_train[\"a\"] if start != end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising the text\n",
    "\n",
    "Now, we'll tokenise the clean text and match up the question and answer ranges so that we can fine-tune a pre-trained DistilBERT model for our task. DistilBERT has a max token length of 512, so we have to also at the same time split up the training text into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"other\", \"question\", \"answer\"]\n",
    "\n",
    "id2label = {k: v for k, v in enumerate(label_list)}\n",
    "label2id = {v: k for k, v in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "STRIDE = 32\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer, tree_q, tree_a):\n",
    "    encodings = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=STRIDE,\n",
    "        add_special_tokens=True,  # Includes the [CLS] and [SEP] tokens\n",
    "    )\n",
    "\n",
    "    all_token_labels = []\n",
    "    for batch_index, (input_ids, offsets) in enumerate(\n",
    "        zip(encodings[\"input_ids\"], encodings[\"offset_mapping\"])\n",
    "    ):\n",
    "        word_ids = encodings.word_ids(batch_index=batch_index)\n",
    "\n",
    "        token_labels = []\n",
    "        current_word_idx = None\n",
    "\n",
    "        for word_id, (start, end) in zip(word_ids, offsets):\n",
    "            if word_id is None:  # Special tokens like [CLS] or [SEP]\n",
    "                token_labels.append(-100)\n",
    "            elif word_id != current_word_idx:  # New word\n",
    "                if len(tree_q.overlap(start, end)) > 0:\n",
    "                    label = \"question\"\n",
    "                elif len(tree_a.overlap(start, end)) > 0:\n",
    "                    label = \"answer\"\n",
    "                else:\n",
    "                    label = \"other\"\n",
    "\n",
    "                token_labels.append(label2id[label])\n",
    "                current_word_idx = word_id\n",
    "            else:  # Subword token\n",
    "                token_labels.append(-100)\n",
    "\n",
    "        all_token_labels.append(token_labels)\n",
    "\n",
    "    encodings[\"labels\"] = all_token_labels\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_dataset = tokenize(clean_train, tokenizer, tree_q, tree_a)\n",
    "\n",
    "training_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": tokenized_dataset[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_dataset[\"attention_mask\"],\n",
    "        \"labels\": tokenized_dataset[\"labels\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great &ndash; now that our data has been prepared, we can inspect the tokens that have been produced and labelled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_ids, labels) in enumerate(  # type: ignore\n",
    "    zip(tokenized_dataset[\"input_ids\"], tokenized_dataset[\"labels\"])  # type: ignore\n",
    "):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    for token, label in zip(tokens, labels):\n",
    "        print(f\"Token: {token:<20} Label: {id2label.get(label)}\")\n",
    "\n",
    "    if i > 32:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a token classification model\n",
    "\n",
    "We are now ready to fine-tune a pre-trained DistilBERT model to perform this token classification task!\n",
    "\n",
    "First, we need to load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoModelForTokenClassification,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been loaded, we are now ready to start fine-tuning it! You may want to experiment with the training arguments (just remember not to accidentally save models you do not want to submit in the `submission/` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=training_dataset,  # You may wish to make your own validation set!\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model has finished training, we can use it to make some predictions for the text we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(\n",
    "    clean_train,\n",
    "    return_offsets_mapping=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    stride=STRIDE,\n",
    "    add_special_tokens=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, (input_ids, attention_mask, offsets) in enumerate(zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"offset_mapping\"])):  # type: ignore\n",
    "        predictions = torch.argmax(\n",
    "            model(input_ids=input_ids, attention_mask=attention_mask).logits, dim=2\n",
    "        )\n",
    "        predicted_token_class = [\n",
    "            model.config.id2label[t.item()] for t in predictions[0]\n",
    "        ]\n",
    "\n",
    "        for cls, offset in zip(predicted_token_class, offsets):\n",
    "            word = clean_train[offset[0] : offset[1]]\n",
    "            print(f\"{word:<20}\", cls)\n",
    "\n",
    "        if i > 8:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producing a submission package\n",
    "\n",
    "**Now, we will move onto creating your first submission!**\n",
    "\n",
    "When you upload your work to the DOXA AI platform, your code will be run in an environment with no internet access. As such, your submission needs to contain any models you want to use as part of the submission, as well as any code necessary to use those models (including tokenisers).\n",
    "\n",
    "Currently, the `submission/` folder contains three files:\n",
    "\n",
    "- `submission/competition.py`: this contains competition-specific code used to interface with the platform\n",
    "- `submission/doxa.yaml`: this is a configuration file used by the DOXA CLI when you make a submission\n",
    "- `submission/run.py`: this is the Python script that gets run when your work gets evaluated (**you will need to edit this to implement your solution!**)\n",
    "\n",
    "First, we will save model and tokeniser into our `submission/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"submission/tokenizer\")\n",
    "trainer.save_model(\"submission/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you upload your submission to the platform, based on the current configuration in `doxa.yaml`, the `run.py` entrypoint file will be run. If you take a look at `run.py`, you will see the following:\n",
    "\n",
    "```py\n",
    "class Evaluator(BaseEvaluator):\n",
    "    def predict(\n",
    "        self, text: str\n",
    "    ) -> Generator[Tuple[int, int, Literal[\"Q\", \"A\"]], Any, None]:\n",
    "        # Load the saved tokeniser and model \n",
    "        tokenizer = AutoTokenizer.from_pretrained(directory / \"tokenizer\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(directory / \"model\")\n",
    "\n",
    "        # Tokenise the input text\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            stride=16,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Chunks overlap, so we want to keep track of predictions we have already made\n",
    "        done = set() \n",
    "\n",
    "        # Produce predictions for each example (in inference mode)\n",
    "        with torch.inference_mode():\n",
    "            for input_ids, attention_mask, offsets in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"offset_mapping\"]):  # type: ignore\n",
    "                predictions = torch.argmax(\n",
    "                    model(input_ids=input_ids, attention_mask=attention_mask).logits,\n",
    "                    dim=2,\n",
    "                )\n",
    "\n",
    "                for t, (start, end) in zip(predictions[0], offsets):\n",
    "                    if (start, end) in done or (start == 0 and end == 0):\n",
    "                        continue\n",
    "\n",
    "                    done.add((start, end))\n",
    "\n",
    "                    predicted_token_class = model.config.id2label[t.item()]\n",
    "                    if predicted_token_class == \"question\":\n",
    "                        yield (start, end, \"Q\")\n",
    "                    elif predicted_token_class == \"answer\":\n",
    "                        yield (start, end, \"A\")\n",
    "```\n",
    "\n",
    "In the `predict()` method, we load the tokeniser and the model we had just been fine-tuning and then use them to produce predictions for the test set. You only need to output where you believe the questions and answers are, and the starting and ending ranges can be larger than a single token (i.e. you could produce a single prediction for a whole question or multiple predictions for each individual word, and the platform will match them up).\n",
    "\n",
    "**When you come to implement your own solution, you will likely need to edit `predict()` in `run.py` to work with your model. Also, make sure you include the right model in your submission!**\n",
    "\n",
    "You can edit `predict()` however you wish, as long as it produces question and answer range predictions that are contained within the document! If your submission is slow to evaluate on the platform, you may wish to edit `predict()` to perform inference in batches rather than chunk by chunk, but this will use more RAM. Note that in addition to the RAM limit, there is a submission size limit, so make sure you are only uploading models that are relevant to your current submisison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading your submission to the platform\n",
    "\n",
    "You are now ready to make your first submission to the platform! ðŸ‘€\n",
    "\n",
    "**Make sure to [enrol to take part](https://doxaai.com/competition/harmony-parsing) in the challenge if you have not already done so.**\n",
    "\n",
    "First, we need to make sure we are logged in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, we can submit our work for evaluation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** ðŸ¥³\n",
    "\n",
    "By this point, you will now have just made your first submission for this challenge on the DOXA AI platform!\n",
    "\n",
    "If everything went well, your submission will now be queued up for evaluation. It will first be run on a small validation set to make sure that your submission does not crash on the full test set. If your submission runs into an issue at this point, you will be able to see the error logs from this phase. Otherwise, if your submission passes this stage, it will be evaluated on the full test set, and you will soon appear on the [competition scoreboard](https://doxaai.com/competition/harmony-parsing/scoreboard)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "**Now, it is up to you as to where you go from here to solve this challenge!**\n",
    "\n",
    "Here are some ideas you might want to test out:\n",
    "\n",
    "- How could you improve the training process to boost performance?\n",
    "- What other [pre-trained models](https://huggingface.co/models?pipeline_tag=token-classification&sort=trending) in HuggingFace transformers could you use?\n",
    "- How could you provide a `compute_metrics` function to the `Trainer` to produce additional metrics? (e.g. accuracy)\n",
    "- How could you make better use of the training data provided?\n",
    "\n",
    "If you are new to fine-tuning language models, take a look at the excellent [HuggingFace `transformers` documentation](https://huggingface.co/docs/transformers/en/training)!\n",
    "\n",
    "**We look forward to seeing what you build!** We would love to hear about what you are working on for this challenge, so do let us know how you are finding the challenge on the [Harmony community Discord server](https://discord.com/invite/harmonydata) or the [DOXA AI community Discord server](https://discord.gg/MUvbQ3UYcf). ðŸ˜Ž\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
